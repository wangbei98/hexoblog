---
title: 课堂笔记-数据挖掘
author: bellick
copyright: true
top: 1
date: 2019-04-25 13:37:51
categories:
- Notebook
- ClassNote
tags:
mathjax:
---


人生苦短，我用Python

### Python 数据分析工具箱

numpy，scipy，pandas，pySpark，，，

```python
type(c)

id(a) 获得地址

dir(a) 查看函数

len()
```

## 第二次作业

要求： Word文本 + markdown 文档

# 5.9 Numpy 

### zip

```python
zip 

for f,s in zip(a,b)
```

### array

```python
a = np.array(a)

b = np.array(b)

a*b # 对应相乘

np.dot(a,b) # 向量内积

np.outer(a,b) ## 向量外积

array = np.array([[1,2,3],[4,5,6]],dtype='float32)

array.dtype

array.ndim

array.shape

array.size # 元素个数

array.flatten() # 扁平化

array.reshape()

```

[矩阵运算](https://bellick.github.io/2018/12/19/pythonDataStructure/)

### 一些初始化函数

```python
np.zeros()

np.ones()

np.empty()

np.random.random()

np.arange(12) #0,1,,,11

np.arange(5,2,-1)

np.arange(1,24,2).reshape(3,4) # 1-2 stride = 2

np.linspace(1,10,9) #start from 1 , stop at 10,total 9 
```

### 计算

```python
a = np.array([1,2,3])

b = np.sin(a) # 对a中的每个元素取sin

// 内积

c = np.dot(a,b)

d = a.dot(b) # a没有改变，返回新的结果
```

### 轴

```python
np.sum(a,axis = 0) # column:0, row:1

# 默认对所有元素求和/min/max

np.sum(a)

np.min(a)

np.max(a)

//算数平均

np.mean(a)

a.mean()

np.average() # 可以计算加权平均

np.median(a) # 中位数

np.cumsum(a) # 累加和,主次累加

np.diff(a)  # 差分
```

### 其他操作

```python
row,col = np.nonzero(a) # 所有不是0的元素的索引

print(list(zip(row,col)))

a[np.nonzero(a)] # 得到所有不为零的元素组成的list

np.argmin(a) # 第一个最小值的下标

np.argmin(a,axis=0)

np.argmin(a,axis=1)

np.sort(a, axis)

np.transpose(a) # 转置

a.T # 转置

a.T.dot(a)

np.clip(a,2,3) # 在区间内的元素不变，小于2->2, 大于3->3

a[a>20] 
```



### 逻辑运算

```python
np.array([True,False])&np.array([True,True])

np.array([True,False])|np.array([True,True])

~np.array([True,False])

//[ True False]

//[ True  True]

//[False  True]

a[(a>30)|(a<=20)]

idx = (a>30)|(a<=20)

print(idx)

print(a[idx])

//[10,20,40]

//[]

idx.any() # 是不是有True 

idx.all() # 是不是全是True
```



### 随机访问

```python
a[2][1]

a[2,1]
```

### 合并&拆分

```python
np.vstack((a,b,c))# 垂直堆叠

np.hstack((a,b)) # 水平堆叠

a = np.array([1,1,1])

a[np.newaxis,:]

[[1,1,1]]

a[:,np.newaxis]

[[1]

 [1]

 [1]]

np.concatenate((a,b,a),axis=0)

np.split(a,2,axis=1)# 沿行的方向 切成两块

np.vsplit(a,2)

np.split(a,3,axis=0)# 沿列的方向，切三块

np.hsplit(a,3)

np.array_split(a,2,axis=0) # 不够切的时候
```

### 拷贝

[拷贝方式](https://bellick.github.io/2019/04/23/Python%E5%A4%8D%E5%88%B6%E3%80%81%E6%B7%B1%E6%8B%B7%E8%B4%9D%E3%80%81%E6%B5%85%E6%8B%B7%E8%B4%9D/)

```python
b = a # 浅拷贝 ， 实际上是引用

b is a # True

b = a.copy() # 深拷贝

b is a # False

// 相当于 Python 中的deepcopy 
```

## 5.13 Pandas

```python
import pandas as pd

df = pd.DataFrame([[][][]])

df.columns = ['a','b','c'] # 设置列名
```

### series

```python
NaN # not a number

phone = pd.Series([111,222,NaN],index = ['a','b','c'])

phone[1]# 按索引

phone['a'] # 按索引名称
```



### 创建DataFrame

1. 二维数组

2. 字典方式 ---每一列是一个列表

```python
df = DataFrame({'name':[],'gender':[],'age':[]},columns = [])
```

3. 每一行是个字典 

```python
df = DataFrame([

​        {'name':'frank','gender':'M','age':12},

​        {'name':'frank','gender':'M','age':12},

​        {'name':'frank','gender':'M','age':12},

​        {'name':'frank','gender':'M','age':12}],

​        columns = [])
pd = DataFrame({

​          'A':1,

​          'B': ,

​          'C': ,

})
```

### 从文件读取

```python
pd.read_csv()

df['new_col'] = True # 增加一列

del df['new_col'] # 删除此行

df.pop('new_col') # 删除此行

df = df.drop('new_col',axis = 1) # 删除 列 ， 返回一个新对象 
```



### index

```python
设置index ，相当于设置主键

df.set_index('name') #选定某一列的值 作为索引， 返回新对象

df.set_index(['name','gender'])

df.set_index(['name','gender'],drop = False) # 同时保留作为索引的列

df.set_index('name',drop = False).set_index('',append =True,drop = False) 

df.reset_index() # 去掉所有的索引

 # 产生一个 date 索引对象

dates = pd.date_range('2019-01-01','2019-01-06')

dates = pd.date_range('2019-01-01',period = 6)

df.index = dates # 将date索引对象当做index
```



### 查看数据

```python
df.describe() 

df.T

df.head()

df.tail()
```



### 增删改查&连接

~~~python
这里默认都是等值连接

 ```

import pandas as pd

dept = pd.read_csv('data/dept.csv')

emp = pd.read_csv('data/emp.csv')

pd.merge(dept,emp)

 # from A join B using(attr)  -- oracle

pd.merge(emp,dept,on='DEPTNO')# 自然连接 on用来设置连接的属性

dept2 = dept.copy()

dept2.columns = ['dno','dname','loc']

pd.merge(dept,dept2,left_on = 'DEPTNO',right_on = 'dno')

 # pd.merge(dept,dept2,left_on = ['DEPTNO',],right_on = ['dno',])

pd.merge(emp,emp,left_on='MGR',right_on='EMPNO')

pd.merge(emp,emp,left_on='MGR',right_on='EMPNO',suffixes = ['_emp','mgr']) # suffixes 设置重名属性的后缀

pd.merge(emp.set_index('DEPTNO'),dept.set_index('DEPTNO'),left_index = True,right_index = True)

pd.merge(emp,dept.set_index('DEPTNO'),left_on = 'DEPTNO',right_index = True)

# 右外连接

pd.merge(emp,dept.set_index('DEPTNO'),left_on = 'DEPTNO',right_index = True,how = 'right')

pd.merge(emp,dept.set_index('DEPTNO'),left_on = 'DEPTNO',right_index = True,how = 'right',indicator = True) # indicator 增加一列，表明数据来自哪张表

# join 

emp.set_index('DEPTNO').join(dept.set_index('DEPTNO'))

# concat

只是按照表的行号匹配 ，默认是outer（简单取并集）

pd.concat([emp,dept],axis = 1,join = 'inner')
~~~



### 数据筛选 切片& 切块 + where + select

- []

- loc,iloc

- ix

- at,iat

dept[2:] # 取某些行

dept['DNAME'] # 取某列

```python
 #label based index 

 #loc 只能用 index 来索引行，index 默认为1，2，3 所以可以用loc[1:4]来查找，当set_index设置索引后，只能用index的值来查找某些行

dept.loc[1]

dept.loc[1:3]# 包含右端点

dept.set_index('DNAME')

dept.loc['SALES'] # 按照主码 取某行

dept.loc[['SALES',],['ENAME','SAL']] # 取SALES 行的这两列

dept.loc[:,['ENAME','SAL']] # 取所有行的这两列
```

```python
position index

iloc 用行号来查找

dept.iloc[1:2,3:4]

 # at 取单个值 运行效率高

dept.set_index['JOB'].at['SALESMAN','ENAME'] # 所有job为salesmen的人的姓名

dept.iat[0,0]
```

### 筛选 where

```python
emp['SAL']>3000

emp[emp['SAL']>3000]

emp[(emp['SAL']>3000) & (emp['JOB']=='MANAGER')] #and

emp[(emp['SAL']>3000) | (emp['JOB']=='MANAGER')] # or

emp[~(emp['JOB']=='MANAGER')] # not

cond = (emp['SAL']>3000) | (emp['JOB']=='MANAGER')]

cond.value_counts()# 获得统计

dept[dept['COMM'].isna()]

dept[dept['COMM'].isnull()] # 等效

dept[dept['COMM'].notna()] # 非空

dept[dept['JOB'].isin(['MANAGER','CLERK'])]
```



### 聚集计算 

```python
emp.groupby('JOB')['SAL'].mean() # 按照job聚集，并统计sal的均值

emp.groupby(['DEPTNO','JOB'])['SAL'].mean()

grp = emp.groupby(['DEPTNO','JOB'])

grp['SAL'].mean()

grp['COMM'].min()

grp['COMM'].count()

grp.agg('mean') # 针对每一列计算聚集函数

grp.agg(['mean','max','size']) # 针对每一列计算多种聚集函数

grp['SAL'].agg(['mean','max','size']) # 针对某一列计算多种聚集函数

grp.agg({'SAL':[np.mean,np.min],'COMM':np.size}) # 不同列进行不同的聚集函数

size 是忽略NaN的大小 相当于 count*

count 是所有元素的个数

emp.groupby(['DEPTNO','JOB']).apply(mean)
```





### GroupBy

```python
df_emp['deptAvgsal'] = df_emp.groupby('DEPTNO')['SAL'].transform('mean')

sal_grps_cond = pd.cut(df_emp['SAL'],bins = [0,1500,3000,np.inf])

df_emp.groupby(sal_grps_cond).count()

# 频次统计，交叉表
pd.crosstab(df_emp['DEPTNO'],df_emp['JOB'])
```

### OrderBY

```python

```



### insert

```python
df_dept3 = pd.concat([df_dept,df_dept3])

df_dept3 = pd.concat([df_dept,df_dept3],ignore_index=True)

# 去重
df_dept3.drop_duplicates()

# insert a row 
new_dept = pd.Series([50,'AAA'],index = ['DEPTNO','DNAME'])
df_dept3 = df_dept3.append(new_dept,ignore_index = True)

new_dept = pd.Series([50,'AAA'])
df_dept3 = df_dept3.append(new_dept,ignore_index = True)
df_dept3

```



### delete

```python
df_dept3.drop(9,axis = 0).drop(10,axis=0)
df_dept3.pop('LOC')

df_emp2 = df_emp.copy()
df_emp2.loc[df_emp['SAL']<1500,'COMM'] = df_emp['SAL']*0.5

# delete
# 相当于删除 sal <= 2000 的值
df_emp2 = df_emp2.loc[df_emp2['SAL']>2000]
```





### 缺失值处理



```python
df_emp.isnull()
df_emp.isnull().values.any() # 看是否有缺失值
df_emp['SAL'].isnull().values.any() # 看某一列是否有缺失值
df_emp.isnull().sum() # 所用列对象的缺失值的个数
df_emp.isnull().sum().sum() # 一共的缺失值个数
df_emp.dropna() # 删除有缺失值的那一行 相当于 df_emp.dropna(how = 'any')
df_emp.dropna(how='all') # 删除 一行全是空的 行
df_emp.dropna(thresh = 7 ,axis = 0) # 按照列的方向处理行， 至少有7个非空值 才会保留/ 小于7个非空值则删除
```



#### 缺失值填充

```python
df_emp.fillna(0)

## 
df_emp.fillna(method='pad') # /ffill 遇到nan找它前面的值填充 
df_emp.fillna(method='pad',limit=2) # 连续的nan 只填充2个
df_emp.fillna(method='bfill') # 按照后面的填充

## 插值 填充
df_emp['COMM'].interpolate() #默认是linear 线性插值
df_emp['COMM'].interpolate(method='nearest')

df_emp['COMM'].fillna(df_emp['SAL'])#用其他列填充
df_emp.fillna(df_emp.groupby('DEPTNO')["COMM"].transform('mean').inplace=True)# 分层填充
```



### 计算

```python
np.sqrt(df_dept['SAL']) #计算一列
df_emp['ENAME'] +'##'+  df_emp['JOB']  # 如果是字符，则进行字符串连接

df_emp['SAL'] + df_emp['COMM'] # 不会处理nan，nan会保留
import math
df['总收入'] = df_emp['SAL'] + df_emp['COMM'].apply(lambda x:0 if math.isnan(x) else x) # 处理 COMM 中的nan数据 
```



### python map reduce & pandas apply 

```
map(lambda x:x**2,[1,2,3])

from functools import reduce
reduce(lambda x,y:x+y,[1,2,3])

fliter()

apply()


nonValue = [' ','','无数据']
df_emp.applymap(lambda x: np.nan if x in nonValue else x)
```



### datetime

```python
from datetime import datetime
current_time = datetime.now()  # datetime.datetime

s = '2019/05-06'
datetime.strptime(s,'%Y/%m-%d')# 将某种特定格式的字符串转换成datetime
current_time.strftime('%Y_%m_%d') # 将datetime 转换成特定格式字符串

from datetime import timedelta
newdat = current_time - timedetla(days=10)# 得到10天前的时间
```

### one-hot 编码 - 哑变量

```python

```

### 正则表达式

```python
import re
# pattern = r'sss'

# 在正则表达式中用 () 来分组

content = 'hello alwejfks 13245443 _ world hfwai;kd 987654 n,v.aregjsdmv.ahgrjs'
result = re.search('(\d+).*?(\d+).*')  #返回第一个匹配的结果，否则返回 None
print(result.group(0))  # 13245443 _ world hfwai;kd 987654 匹配结果
print(reslut.group(1))  # 13245443 
print(reslut.group(2))  # 987654

# search 必须从字符串头开始搜索
# match 

email = 'hello@126.com'
m = re.match('\w+@\w+') # \w: 0-9 a-z A-Z
m.group() # hello@126
m.group(1) # error 表达式中未分组

m = re.match('(\w+)@(\w+)') # \w: 0-9 a-z A-Z
m.group() # hello@126
m.group(1) # hello
m.group(2) # 126

re.match('\P<domain>(\w+)@\p<site>(\w+)') # 为每个分组 命名
```
