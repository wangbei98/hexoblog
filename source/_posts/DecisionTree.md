---
title: DecisionTree
author: bellick
copyright: true
top: 0
date: 2018-11-14 19:49:28
categories:
- MachineLearning
- Models
tags:
- LinearRegression
- ML
mathjax: true
---

# 决策树
## 1. 决策树原理

## 2. 最优划分属性
#### 纯度（不纯度）
常用度量指标包括:

* 信息增益(information gain )，
* 增益率(gain ratio)，
* 基尼指数(Gini index)}

分别对应了三种决策树算法:

* 信息增益:ID3，
* 增益率:C4.5，
* 基尼指数:CART

#### 信息熵
学习信息增益，首先要了解什么是信息熵，假定当 前样本集合𝐷中第𝑘类样本所占的比例为 \\(p_k (k = 1,2,..,|y|)\\) ，则𝐷的信息熵定义为:
$$Ent(D) = -\sum_{k=1}^{|Y|}p_klog_2p_k$$

信息熵越小，则𝐷的纯度越高，信息熵越大，包含的 信息越大，不纯度越高

熵在信息论中代表随机变量不确定度的度量，简单地说，信息熵就是衡量信息量的度量。
#### 信息熵、信息增益与ID3
假定离散属性𝑎有𝑉个可能的取值\\(\{a^1,a^2,...,a^V\}\\)，若 使用𝑎来对样本集𝐷进行划分，则会产生𝑉个分支结点， 其中第𝑣个分支结点包含了𝐷中所有在属性𝑎上取值为𝑎𝑣 的样本，记为𝐷𝑣。则属性𝑎对样本集𝐷进行划分所得的“ 信息增益”为:

$$Gain(D,a) = Ent(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$$

通常来说，信息增益越大，则意味着使用属性𝑎来进 行划分所获得的“纯度提升”越大
#### 增益率与C4.5
增益率定义：
$$Gain\_ratio = \frac{Gain(D,a)}{IV(a)}$$

其中：
$$IV(a) = -\sum_{v=1}^V\frac{D^v}{D}log_2\frac{D^v}{D}$$

称为属性𝑎的“固有值”。属性𝑎的可能取值数目越多，即𝑉越大，则𝐼𝑉(𝑎)的值通常会越大。

C4.5算法不是直接选择增益率最大的属性作为最优 划分属性，而是先从候选划分属性中，找出信息增益高 于平均水平的属性，再从中选择增益率最高的属性。
#### 基尼指数与CART
$$Gini = \sum_{k=1}^{|Y|}\sum_{k'!=k}p_kp_{k'} = 1-\sum_{k=1}^{|y|}p_k^2$$

CART决策树使用“基尼指数”(Gini index)来选择划 分属性，𝐺𝑖𝑛𝑖(𝐷)越小，则数据集𝐷的纯度越高。
## 3. 过拟合与剪枝
先生成一棵完整的决策树，然后依次从底部判断是否剪除结点
## 4. 连续值、缺失值处理
#### 连续值处理
决策树中处理连续属性，需要对连续属性进行离散
化。通常来说，样本数量是有限的，因此尽管连续变量
取值是连续的，但其样本值是分布在数轴上的点，因此
也可以看作是“离散”的。
#### 缺失值处理
如果数据集足够大，可以假定缺失值分布和非缺失
值分布一致，因此缺失值的信息，可以计算非缺失值的
数据信息，然后按照缺失值比例进行加权。
## 5. 超参数与格搜索
